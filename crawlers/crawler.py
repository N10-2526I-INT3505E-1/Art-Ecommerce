import requests
from bs4 import BeautifulSoup
import time
import random
import json

# Link trang danh m·ª•c (v√≠ d·ª•: Tranh phong c·∫£nh v√πng cao)
BASE_URL = "https://bantranh.com"
LIST_URL = "https://bantranh.com/pc/tranh-phong-canh-vung-cao/page/{}/" # Trang n√†y c√≥ th·ªÉ kh√¥ng ph√¢n trang ki·ªÉu ?page=1, c·∫ßn ki·ªÉm tra k·ªπ
API_URL = "http://localhost:3000/products"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# --- T·ª™ ƒêI·ªÇN PHONG TH·ª¶Y (ƒê·ªÉ sinh Tags cho AI) ---
TAG_RULES = {
    "hoa": [
        "m·∫∑t tr·ªùi", "b√¨nh minh", "ho√†ng h√¥n", "√°nh d∆∞∆°ng",
        "ƒë·ªè", "cam", "n·∫Øng", "chi·ªÅu t√†",
        "ng·ª±a", "m√£ ƒë√°o", "ph∆∞·ª£ng ho√†ng"
    ],
    "thuy": [
        "bi·ªÉn", "b·ªù bi·ªÉn", "s√≥ng", "thuy·ªÅn", "thu·∫≠n bu·ªìm",
        "th√°c", "th√°c n∆∞·ªõc", "s√¥ng", "su·ªëi", "h·ªì",
        "m∆∞a", "ƒë√™m bi·ªÉn", "ban mai bi·ªÉn"
    ],
    "moc": [
        "c√¢y", "r·ª´ng", "r·ª´ng th√¥ng", "th√¥ng", 
        "ƒë·ªìi ch√®", "ƒë·ªìi", "hoa", "v∆∞·ªùn hoa",
        "m√πa xu√¢n", "ƒë·ªìi th√¥ng", "v√πng cao",
        "ru·ªông b·∫≠c thang", "m√πa l√∫a ch√≠n"
    ],
    "kim": [
        "tuy·∫øt", "tr·∫Øng", "bƒÉng", 
        "√°nh b·∫°c", "kim lo·∫°i", "c√¥ng", "thi√™n nga"
    ],
    "tho": [
        "n√∫i", "ƒë·ªìi n√∫i", "n√∫i non", "non n∆∞·ªõc",
        "hang ƒë·ªông", "ƒë√°", "ƒë√®o", 
        "l√†ng qu√™", "ru·ªông", "c√°nh ƒë·ªìng",
        "n√¢u", "ƒë·∫•t", "ph·ªë c·ªï", "ƒë·ªÅn"
    ],

        "tai_loc": [
        "thu·∫≠n bu·ªìm", "c·ª≠u ng∆∞", "s√≥ng bi·ªÉn",
        "l√∫a ch√≠n", "m√πa g·∫∑t", "ƒë·ªìi ch√®",
        "ru·ªông b·∫≠c thang", "v√†ng", "√°nh v√†ng"
    ],

    "cong_danh": [
        "ƒë·ªânh n√∫i", "n√∫i cao", "m·∫∑t tr·ªùi m·ªçc",
        "ƒë·∫°i b√†ng", "ng·ª±a", "th√°c n∆∞·ªõc l·ªõn",
        "√°nh s√°ng m·∫°nh", "tuy·∫øt n√∫i"
    ],

    "binh_an": [
        "l√†ng qu√™", "c√°nh ƒë·ªìng", "hoa sen",
        "ph·∫≠t", "su·ªëi nh·ªè", "thung l≈©ng",
        "c√¢y ƒë∆°n", "nh√† g·ªó", "tr·ªùi xanh"
    ],

    "tinh_duyen": [
        "ƒë√¥i chim", "thi√™n nga", "uy√™n ∆∞∆°ng",
        "c·∫∑p ƒë√¥i", "hoa h·ªìng", "ƒë√™m paris",
        "√°nh ƒë√®n ƒë∆∞·ªùng", "m√πa thu l√£ng m·∫°n"
    ],

    "hien_dai": [
        "hi·ªán ƒë·∫°i", "3d", "s∆°n d·∫ßu hi·ªán ƒë·∫°i", "tr·ª´u t∆∞·ª£ng phong c·∫£nh",
        "b·∫Øc √¢u", "scandinavia", "minimalist", "t·ªëi gi·∫£n",
        "ph·∫£n chi·∫øu", "phong c·∫£nh tr·ª´u t∆∞·ª£ng", "ƒë∆∞·ªùng n√©t"
    ],

    "co_dien": [
        "s∆°n d·∫ßu c·ªï ƒëi·ªÉn", "th·ªßy m·∫∑c", 
        "phong c·∫£nh x∆∞a", "l√†ng c·ªï", "ph·ªë c·ªï",
        "c·∫ßu ng√≥i", "phong c√°ch √° ƒë√¥ng"
    ],

    "lang_man": [
        "m√πa thu", "l√° v√†ng", "hoa", "c√¥ng vi√™n",
        "paris", "√°nh ƒë√®n", "ƒë√™m", "ƒë·ªìi hoa"
    ],

        "phong_canh_nui": [
        "n√∫i", "ƒë·ªìi n√∫i", "ƒë·ªânh n√∫i", "r·ª´ng n√∫i",
        "m√πa ƒë√¥ng tuy·∫øt", "d√£y n√∫i", "ƒë√®o"
    ],

    "phong_canh_bien": [
        "bi·ªÉn", "b·ªù bi·ªÉn", "h·∫£i ƒëƒÉng", "thuy·ªÅn bu·ªìm",
        "s√≥ng", "c√°t tr·∫Øng", "bi·ªÉn ƒë√™m"
    ],

    "phong_canh_lang_que": [
        "l√†ng qu√™", "c√°nh ƒë·ªìng", "ru·ªông", 
        "tr√¢u", "nh√† tranh", "c·ªïng l√†ng", "ƒë·ªìng l√∫a"
    ],

    "phong_canh_chau_au": [
        "paris", "ch√¢u √¢u", "ƒë∆∞·ªùng ph·ªë t√¢y", 
        "c·∫ßu ch√¢u √¢u", "th√°p eiffel", "tuy·∫øt ch√¢u √¢u"
    ],

    "phong_canh_thac_nuoc": [
        "th√°c", "th√°c n∆∞·ªõc", "d√≤ng ch·∫£y", "su·ªëi tr·∫Øng"
    ],

    "tone_xanh": ["xanh d∆∞∆°ng", "xanh bi·ªÉn", "xanh l√°", "r·ª´ng xanh"],
    "tone_vang": ["v√†ng", "l√∫a ch√≠n", "m√πa thu", "n·∫Øng v√†ng"],
    "tone_do": ["ƒë·ªè", "ho√†ng h√¥n", "m·∫∑t tr·ªùi", "cam"],
    "tone_trang": ["tr·∫Øng", "tuy·∫øt", "bƒÉng", "s∆∞∆°ng"],
    "tone_den": ["ƒëen", "b√≥ng ƒë√™m", "b·∫ßu tr·ªùi ƒë√™m"],

    "mua_xuan": ["xu√¢n", "hoa ƒë√†o", "m√†u h·ªìng", "n·∫£y l·ªôc"],
    "mua_he": ["h√®", "r·ª±c n·∫Øng", "hoa h∆∞·ªõng d∆∞∆°ng"],
    "mua_thu": ["m√πa thu", "l√° v√†ng", "l√° ƒë·ªè"],
    "mua_dong": ["m√πa ƒë√¥ng", "tuy·∫øt", "l·∫°nh", "tr·ªùi x√°m"],

    "tay_bac": [
    "ru·ªông b·∫≠c thang", "n√∫i r·ª´ng t√¢y b·∫Øc",
    "nh√† s√†n", "b·∫£n l√†ng", "m√πa l√∫a ch√≠n"
    ],

    "treo_phong_khach": [
    "tranh ph√≤ng kh√°ch", "ph√≤ng kh√°ch", "kh·ªï l·ªõn"
    ],
    "treo_phong_ngu": [
        "ph√≤ng ng·ªß", "d·ªÖ ch·ªãu", "√™m d·ªãu", "l√£ng m·∫°n"
    ],
    "treo_phong_lam_viec": [
        "th√°c n∆∞·ªõc", "n√∫i cao", "ƒë·ªânh n√∫i", "thuy·ªÅn bu·ªìm"
    ],
}



def generate_tags(text):
    """Sinh tags t·ª´ t√™n danh m·ª•c/t√™n tranh"""
    text = text.lower()
    tags = []
    for key, keywords in TAG_RULES.items():
        for kw in keywords:
            if kw in text:
                if key in ["hoa", "thuy", "moc", "kim", "tho"]:
                    tags.append(f"phong_thuy_{key}")
                else:
                    tags.append(f"cau_{key}")
                break
    return list(set(tags))

# ----------------------------------------
# 1. Crawler
# ----------------------------------------
def get_product_links(page):
    print(f"üü¶ ƒêang t·∫£i trang danh s√°ch: {LIST_URL}")
    url = LIST_URL.format(page)
    res = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")

    product_links = []

    cnt = 0
    copy_href = ""
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "/pd" in href and copy_href != href:
            print(href)
            product_links.append(href)
            copy_href = href
            cnt += 1
    print(cnt)
        
    return product_links[:1]

def get_product_detail(url):
    time.sleep(random.uniform(0.5, 1.5))
    res = requests.get(url, headers=HEADERS)
    soup = BeautifulSoup(res.text, "html.parser")

    title = soup.find("h1")
    title = title.text.strip() if title else "Untitle"

    price_tag = soup.select_one(".price") # Class .price ph·ªï bi·∫øn ·ªü bantranh.com
    price_raw = price_tag.text.strip() if price_tag else "0"
    
    try:
        clean_price = float(price_raw.replace('.', '').replace(',', '').replace('‚Ç´', '').replace('vnƒë', '').strip())
    except:
        clean_price = 0.0

    img_tag = soup.find("img", class_="wp-post-image skip-lazy")
    img_url = img_tag["src"] if img_tag else ""
    if img_url and not img_url.startswith("http"):
        img_url = "https:" + img_url # X·ª≠ l√Ω n·∫øu link thi·∫øu https

    category = "tranh phong c·∫£nh v√πng cao"
    auto_tags = generate_tags(f"{title} {category}")

    return {
        "name": title,
        "price": clean_price,
        "imageUrl": img_url,
        "categoryName": category,
        "tags": auto_tags,
        "description": f"Crawl from {url}",
        "sourceUrl": url
    }

links = get_product_links(1)
print(links)
print(f"Find {len(links)} products")

for link in links:
    print("-> crawl:", link)
    try:
        data=get_product_detail(link)
        print(json.dumps(data, ensure_ascii=False, indent=2))


        resp = requests.post(API_URL, json=data)
        if resp.status_code != 201:
            print(f"    ‚ùå L·ªói API: {resp.text}")
    except Exception as e:
        print(e)
    
    


